{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import json\n",
    "import pandas as pd\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "from ltp import LTP\n",
    "import re\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "jieba.analyse.set_stop_words(\"./data/hit_stopwords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_text_path = './data/annotations/labeled.json'\n",
    "test_text_path = './data/annotations/test_a.json'\n",
    "unlabeled_text_path = './data/annotations/unlabeled.json'\n",
    "save_path = './temp_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 拼接title_ocr_asr\n",
    "def load_text_data(file_path, file_type, save_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = json.load(f)\n",
    "    df_text = pd.DataFrame(text)\n",
    "    all_ocr_text = []\n",
    "    all_time = []\n",
    "    for ocr in tqdm(df_text['ocr']):\n",
    "        all_ocr_text.append(', '.join([i['text'] for i in ocr]))\n",
    "        all_time.append([i['time'] for i in ocr])\n",
    "    df_text['ocr_text'] = all_ocr_text\n",
    "    df_text['ocr_time'] = all_time\n",
    "#     df_text['text'] =  df_text['title'].str[:64].astype(str) + df_text['title'].str[-64:].astype(str) +  '[SEP]' +\\\n",
    "#                        df_text['asr'].str[0:64].astype(str) + df_text['asr'].str[-64:].astype(str) + '[SEP]' +  \\\n",
    "#                        df_text['ocr_text'].str[:64].astype(str) + df_text['ocr_text'].str[-64:].astype(str)\n",
    "    df_text['asr_ocr_text'] = df_text['asr'].str[0:64].astype(str) + df_text['asr'].str[-64:].astype(str) + \\\n",
    "                       df_text['ocr_text'].str[:64].astype(str) + df_text['ocr_text'].str[-64:].astype(str)\n",
    "    df_text['all_text'] = df_text['title'].astype(str) + df_text['asr'].astype(str) + df_text['ocr_text'].astype(str)\n",
    "    \n",
    "    if file_type == 'train':\n",
    "        df_text = df_text[['id', 'title', 'asr_ocr_text', 'all_text', 'category_id']]\n",
    "    elif file_type == 'unlabeled' or file_type == 'test':\n",
    "        df_text = df_text[['id', 'title', 'asr_ocr_text', 'all_text']]\n",
    "    df_text.to_pickle(save_path + file_type + '_text.pkl')\n",
    "    print(len(df_text))\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ba511f2fb84c52896f4e486b3deb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb52291e2984ea58f04c34a489f16a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "load_text_data(train_text_path, file_type='train', save_path=save_path)\n",
    "load_text_data(test_text_path, file_type='test', save_path=save_path)\n",
    "# load_text_data(unlabeled_text_path, file_type='unlabeled', save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def stopwords_list(filepath):\n",
    "    stopwords = [line.strip() for line in open(filepath, 'r',\n",
    "                                               encoding='utf-8').readlines()]\n",
    "    stopwords.append(\"\\n\")\n",
    "    stopwords.append(\" \")\n",
    "    return stopwords\n",
    "\n",
    "def get_clean_text(text_list, stopwords):\n",
    "    clean_text_list = []\n",
    "    for text in tqdm(text_list):\n",
    "        text = re.sub(\"[^\\u4e00-\\u9fa5。？．，！：]\", \"\", text.strip())\n",
    "        clean_text = [i for i in text if i not in stopwords]\n",
    "        clean_text = ''.join(clean_text)\n",
    "        clean_text_list.append(clean_text)\n",
    "    return clean_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 25000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_pickle(\"./temp_data/train_text.pkl\")\n",
    "df_test = pd.read_pickle(\"./temp_data/test_text.pkl\")\n",
    "train_text = df_train['all_text'].tolist()\n",
    "test_text = df_test['all_text'].tolist()\n",
    "len(train_text), len(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86fdb369d4da4cdea95c845b2e132dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f133113fb341828faa78f9af2dde49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stopwords = stopwords_list('./data/hit_stopwords.txt')\n",
    "train_clean_text = get_clean_text(train_text, stopwords)\n",
    "test_clean_text = get_clean_text(test_text, stopwords)\n",
    "df_train['clean_text'] = train_clean_text\n",
    "df_test['clean_text'] = test_clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file ./ltp-model-small\\config.json not found\n",
      "file ./ltp-model-small\\config.json not found\n"
     ]
    }
   ],
   "source": [
    "ltp = LTP(path='./ltp-model-small')\n",
    "def get_clean_word_list(text_list):\n",
    "    clean_word_lists = []\n",
    "    for text in tqdm(text_list):\n",
    "        clean_text = ltp.seg([text])[0][0]\n",
    "        clean_text = [i for i in clean_text if i not in  stopwords]\n",
    "        clean_word_lists.append(clean_text)\n",
    "    return clean_word_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547202ba54fb48a59eb4ed71e0c6cfcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282160155c00407d879ff000fff25a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_clean_word_list = get_clean_word_list(train_clean_text)\n",
    "test_clean_word_list = get_clean_word_list(test_clean_text)\n",
    "df_train['clean_word'] = train_clean_word_list\n",
    "df_test['clean_word'] = test_clean_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topK_word(text_list, topK):\n",
    "    keyword_list = []\n",
    "    for text in tqdm(text_list):\n",
    "        keyword = jieba.analyse.textrank(text,\n",
    "                                         topK=topK,\n",
    "                                         allowPOS=('n','nz','v','vd','vn','l','a','d'))\n",
    "        keyword_list.append(keyword)\n",
    "    return keyword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1ae354ad2742eeb821ceb180a274d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\B302\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.620 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21cdfa10a1a149acb6414d63a2429107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_key_word = get_topK_word(train_text, topK=20)\n",
    "test_key_word = get_topK_word(test_text, topK=20)\n",
    "df_train['keywords'] = train_key_word\n",
    "df_test['keywords'] = test_key_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'title', 'asr_ocr_text', 'all_text', 'category_id', 'clean_text',\n",
      "       'clean_word', 'keywords'],\n",
      "      dtype='object')\n",
      "Index(['id', 'title', 'asr_ocr_text', 'all_text', 'clean_text', 'clean_word',\n",
      "       'keywords'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_train.columns)\n",
    "print(df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_pickle(\"./temp_data/new_train_text.pkl\")\n",
    "df_test.to_pickle(\"./temp_data/new_test_text.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle(\"./temp_data/new_train_text.pkl\")\n",
    "df_test = pd.read_pickle(\"./temp_data/new_test_text.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train))\n",
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_keyword(x):\n",
    "    return ' '.join(x)\n",
    "def get_title_top20(df):\n",
    "    df['keyword_sentence'] = df['keywords'].apply(lambda x:concat_keyword(x))\n",
    "    df['title_top20'] = df['title'].astype(str) + '[CLS]' + df['keyword_sentence'].astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = get_title_top20(df_train)\n",
    "df_test = get_title_top20(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_pickle(\"./temp_data/new_train_text.pkl\")\n",
    "df_test.to_pickle(\"./temp_data/new_test_text.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'title', 'asr_ocr_text', 'all_text', 'category_id', 'clean_text',\n",
       "       'clean_word', 'keywords', 'keyword_sentence', 'title_top20'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
